{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp-text-preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shambhavithakur/nlp-text-preprocessing/blob/main/nlp-text-preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJbdQU3jQEMB"
      },
      "source": [
        "# Natural Language Processing Techniques\n",
        "## Preprocessing text\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVcD02icOkIv",
        "outputId": "26449cd0-7f46-4502-cd2c-bcbb4d0abf90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install autocorrect"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: autocorrect in /usr/local/lib/python3.6/dist-packages (2.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqMBu-vAOrFP",
        "outputId": "1d97015a-5564-4afd-8c12-14328897fef4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from autocorrect import Speller\n",
        "from nltk import word_tokenize, download\n",
        "\n",
        "download(['punkt', 'averaged_perceptron_tagger'])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLQUDceLA1Po"
      },
      "source": [
        "import re"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EanAwAvBA_j"
      },
      "source": [
        "# Getting text from file\n",
        "\n",
        "with open('/content/file.txt') as f:\n",
        "    sentence_list = [line.strip() for line in f.readlines()]\n",
        "    sentences = ' '.join(sentence_list)\n",
        "    sentences = re.sub(r'\\s*[.]\\s*', '. ', re.sub(r'\\s*[,]\\s*',\\\n",
        "                            ', ', sentences)).strip()    "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1e30L_pBpnQ"
      },
      "source": [
        "# Converting text to tokens\n",
        "\n",
        "tokens = word_tokenize(sentences)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVcwS1zLCLBo",
        "outputId": "74e73a13-62fc-4ca3-f25f-31343f436dca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "# Printing first 20 tokens\n",
        "\n",
        "tokens[:20]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'reader',\n",
              " 'of',\n",
              " 'this',\n",
              " 'course',\n",
              " 'should',\n",
              " 'have',\n",
              " 'a',\n",
              " 'basic',\n",
              " 'knowledge',\n",
              " 'of',\n",
              " 'the',\n",
              " 'Python',\n",
              " 'programming',\n",
              " 'lenguage',\n",
              " '.',\n",
              " 'He/she',\n",
              " 'must',\n",
              " 'have',\n",
              " 'knowldge']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlh565jiCdyM"
      },
      "source": [
        "# Preparing to autocorrect tokens\n",
        "# Creating a spelling-corrector instance\n",
        "\n",
        "spell = Speller(lang='en')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cXk4HicJOfl"
      },
      "source": [
        "# Autocorrecting the tokens\n",
        "\n",
        "tokens_corrected = [spell(word) for word in tokens]"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_EZGZxTJWzm",
        "outputId": "d8fe8275-f6bb-4829-b431-6dbabded4103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "# Printing the first 20 corrected tokens\n",
        "\n",
        "tokens_corrected[:20]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'reader',\n",
              " 'of',\n",
              " 'this',\n",
              " 'course',\n",
              " 'should',\n",
              " 'have',\n",
              " 'a',\n",
              " 'basic',\n",
              " 'knowledge',\n",
              " 'of',\n",
              " 'the',\n",
              " 'Python',\n",
              " 'programming',\n",
              " 'language',\n",
              " '.',\n",
              " 'He/she',\n",
              " 'must',\n",
              " 'have',\n",
              " 'knowledge']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvb6CuRKDcts"
      },
      "source": [
        "# Combining the tokens\n",
        "\n",
        "sentences_corrected = ' '.join(tokens_corrected)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoYpKMWCOumV"
      },
      "source": [
        "sentences_corrected = re.sub(r'\\s*[.]\\s*', '. ',re.sub(r'\\s*[,]\\s*',\\\n",
        "                            ', ', sentences_corrected)).strip()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXGEMVNHD6KR",
        "outputId": "8d69c8d7-150d-46c4-da8c-83520d4522a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "sentences_corrected"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The reader of this course should have a basic knowledge of the Python programming language. He/she must have knowledge of data types in Python. He should be able to write functions, and also have the ability to import and use libraries and packages in Python. Familiarity with basic linguistics and probability is assumed although not required to fully complete this course.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0BAsn6NJzoi"
      },
      "source": [
        "# Applying parts-of-speech (PoS) tags to the corrected tokens\n",
        "\n",
        "from nltk import pos_tag"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrOdCE7UKtPe"
      },
      "source": [
        "pos_tagged = pos_tag(tokens_corrected)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8jsrpexLAEf",
        "outputId": "8df98705-3ced-44a8-cda7-54577ffd7f22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "# Printing first 5 tokens tagges with PoS\n",
        "\n",
        "pos_tagged[:5]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('reader', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('this', 'DT'),\n",
              " ('course', 'NN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJZfJM5RLazy",
        "outputId": "3bfc9a9f-9d5a-497c-ea12-7276959facf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Preparing to remove stopwords \n",
        "# Importing relevant module\n",
        "\n",
        "download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkUVZVgLL-af"
      },
      "source": [
        "# Saving stopwords in a variable\n",
        "\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7lwsIkPMU8S"
      },
      "source": [
        "# Tokens without stopwords\n",
        "\n",
        "tokens_no_stopwords = [word for word in tokens_corrected if word not in stop_words]"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADkdkwePMhuj",
        "outputId": "6f291b50-538b-4ac7-fc52-e5db5f2ccc7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "# Printing the first 20 tokens with stopwords removed\n",
        "\n",
        "tokens_no_stopwords[:20]"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'reader',\n",
              " 'course',\n",
              " 'basic',\n",
              " 'knowledge',\n",
              " 'Python',\n",
              " 'programming',\n",
              " 'language',\n",
              " '.',\n",
              " 'He/she',\n",
              " 'must',\n",
              " 'knowledge',\n",
              " 'data',\n",
              " 'types',\n",
              " 'Python',\n",
              " '.',\n",
              " 'He',\n",
              " 'able',\n",
              " 'write',\n",
              " 'functions']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QEvsz5wMual"
      },
      "source": [
        "# Stemming or converting words into their base versions\n",
        "\n",
        "from nltk import stem\n",
        "\n",
        "stemmer = stem.SnowballStemmer('english')\n",
        "\n",
        "tokens_stemmed = [stemmer.stem(word) for word in tokens_no_stopwords]"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "me8i0c4OPwit",
        "outputId": "f02404b5-2254-4cee-fd10-39ee9e62bd27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "tokens_stemmed[:20]"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'reader',\n",
              " 'cours',\n",
              " 'basic',\n",
              " 'knowledg',\n",
              " 'python',\n",
              " 'program',\n",
              " 'languag',\n",
              " '.',\n",
              " 'he/sh',\n",
              " 'must',\n",
              " 'knowledg',\n",
              " 'data',\n",
              " 'type',\n",
              " 'python',\n",
              " '.',\n",
              " 'he',\n",
              " 'abl',\n",
              " 'write',\n",
              " 'function']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbHrqE7rQD0-",
        "outputId": "5d0e5c4e-22b1-4368-d04c-eb6584e432fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Preparing to lemmatize the tokens\n",
        "\n",
        "download('wordnet')\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Cn6lc26QCth"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeqL6o-5QtIa"
      },
      "source": [
        "# Creating a lemmatizer instance\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVv_shtMQ35D"
      },
      "source": [
        "# Lemmatizing the tokens\n",
        "\n",
        "tokens_lemmatized = [lemmatizer.lemmatize(word) for word in tokens_no_stopwords]"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf5nPGZIRMf8",
        "outputId": "59754d48-9a6f-41d6-9f7d-6f054d8b849c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "# Printing the first 20 lemmatized tokens\n",
        "\n",
        "tokens_lemmatized[:20]"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'reader',\n",
              " 'course',\n",
              " 'basic',\n",
              " 'knowledge',\n",
              " 'Python',\n",
              " 'programming',\n",
              " 'language',\n",
              " '.',\n",
              " 'He/she',\n",
              " 'must',\n",
              " 'knowledge',\n",
              " 'data',\n",
              " 'type',\n",
              " 'Python',\n",
              " '.',\n",
              " 'He',\n",
              " 'able',\n",
              " 'write',\n",
              " 'function']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uOd8_ecRXf7",
        "outputId": "ad6fb6c5-6bf7-4f21-f0a7-c1f69ca20695",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Combining the lemmatized tokens into a text chunk\n",
        "\n",
        "sentences_lemmatized = ' '.join(tokens_lemmatized)\n",
        "sentences_lemmatized = re.sub(r'\\s*[.]\\s*', '. ',re.sub(r'\\s*[,]\\s*',\\\n",
        "                            ', ', sentences_lemmatized)).strip()\n",
        "\n",
        "sentences_lemmatized"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The reader course basic knowledge Python programming language. He/she must knowledge data type Python. He able write function, also ability import use library package Python. Familiarity basic linguistics probability assumed although required fully complete course.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA8GNNroS2lL"
      },
      "source": [
        "# Detecting sentence boundaries\n",
        "\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1DZzj0AZZUl"
      },
      "source": [
        "sentence_boundaries = sent_tokenize(sentences_corrected)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28kbd9VkTCNi",
        "outputId": "7f820738-1272-4873-f9eb-37e766010c62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(sentence_boundaries)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sQ5Ya4xZ4ka",
        "outputId": "148e2175-0010-4510-cbc0-0421b484d3ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "sentence_boundaries"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The reader of this course should have a basic knowledge of the Python programming language.',\n",
              " 'He/she must have knowledge of data types in Python.',\n",
              " 'He should be able to write functions, and also have the ability to import and use libraries and packages in Python.',\n",
              " 'Familiarity with basic linguistics and probability is assumed although not required to fully complete this course.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    }
  ]
}